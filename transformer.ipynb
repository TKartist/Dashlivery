{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date, when, col\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e163e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_ea(sheet):\n",
    "    # Take the EA sheet, build a unique reference ID for each row,\n",
    "    # then split it into 3 smaller tables: disasters, operational progress, and DREF shift info.\n",
    "    col_name = sheet.columns.tolist()\n",
    "    sheet[\"Ref\"] = \"EA\" + sheet[col_name[4]] + sheet[col_name[6]]\n",
    "    disasters = sheet.loc[:, col_name[:11] + [col_name[75]]]\n",
    "    ops_details = sheet.loc[:, [col_name[0]] + col_name[11:16] + col_name[22:26] + col_name[38:58] + col_name[69:70] + col_name[71:75] + col_name[76:81]]\n",
    "\n",
    "    dref_shift = sheet.loc[:, [col_name[0]] + [col_name[70]]]\n",
    "    print(\"EA master data organized\")\n",
    "    return {\"disasters\" : disasters.set_index(\"Ref\"), \"operational_progresses\" : ops_details.set_index(\"Ref\"), \"dref_shift\" : dref_shift.set_index(\"Ref\")}\n",
    "\n",
    "def organize_dref(sheet):\n",
    "    # Same idea as organize_ea but for DREF data: add reference ID, and split into disasters and operational progress tables.\n",
    "    col_name = sheet.columns.tolist()\n",
    "    sheet[\"Ref\"] = \"DREF\" + sheet[col_name[4]] + sheet[col_name[6]]\n",
    "    disasters = sheet.loc[:, col_name[:11] + [col_name[52]]]\n",
    "    operational_progresses = sheet.loc[:, [col_name[0]] + col_name[11:19] + col_name[32:39] + col_name[51:52] + col_name[53:56]]\n",
    "\n",
    "    print(\"DREF master data organized\")\n",
    "    return {\"disasters\": disasters.set_index(\"Ref\"), \"operational_progresses\": operational_progresses.set_index(\"Ref\")}\n",
    "\n",
    "def organize_dref_escalated(sheet):\n",
    "    # Organize DREF escalated sheet: set reference ID, tag it as \"DREF 2nd Allocation\", and split into disasters + progress.\n",
    "    col_name = sheet.columns.tolist()\n",
    "    sheet[\"Ref\"] = \"DREF\" + sheet[col_name[4]] + sheet[col_name[6]]\n",
    "    sheet[\"EWTS Varient_\"] = \"DREF 2nd Allocation\"\n",
    "    disasters = sheet.loc[:, col_name[:11] + [col_name[49]]]\n",
    "    operational_progresses = sheet.loc[:, [col_name[0]] + col_name[11:17] + col_name[30:37]]\n",
    "    print(\"DREF master data organized\")\n",
    "    return {\"disasters\": disasters.set_index(\"Ref\"), \"operational_progresses\": operational_progresses.set_index(\"Ref\")}\n",
    "\n",
    "def organize_mcmr(sheet):\n",
    "    # Organize multi-country (MCMR) sheet: create ref ID and split into disasters + operational progress tables.\n",
    "    col_name = sheet.columns.tolist()\n",
    "    sheet[\"Ref\"] = \"MCMR\" + sheet[col_name[6]] \n",
    "    disasters = sheet.loc[:, col_name[:11] + [col_name[53]]]\n",
    "    operational_progresses = sheet.loc[:, [col_name[0]] + col_name[11:14] + col_name[20:23] + col_name[36:45] + col_name[47:53]]\n",
    "\n",
    "    print(\"MCMR master data organized\")\n",
    "    return {\"disasters\" : disasters.set_index(\"Ref\"), \"operational_progresses\" : operational_progresses.set_index(\"Ref\")}\n",
    "\n",
    "\n",
    "def organize_protracted(sheet):\n",
    "    # Organize protracted crisis (PCCE) sheet: create ref ID and split into disasters + operational progress tables.\n",
    "    col_name = sheet.columns.tolist()\n",
    "    sheet[\"Ref\"] = \"PCCE\" + sheet[col_name[4]] + sheet[col_name[6]]\n",
    "    disasters = sheet.loc[:, col_name[:11]]\n",
    "    operational_progresses = sheet.loc[:, [col_name[0]] + col_name[11:15] + col_name[21:25] + col_name[39:57] + col_name[57:58] + col_name[60:65]]\n",
    "\n",
    "    print(\"PCCE master data organized\")\n",
    "    return {\"disasters\" : disasters.set_index(\"Ref\"), \"operational_progresses\" : operational_progresses.set_index(\"Ref\")}\n",
    "\n",
    "def organize_sheets():\n",
    "    # Read the different Spark tables into pandas, organize each type,\n",
    "    # and return everything in one big dictionary called bucket.\n",
    "    bucket = {}\n",
    "    bucket[\"DREF\"] = organize_dref(spark.read.table(\"dref\").toPandas())\n",
    "    bucket[\"EA\"] = organize_ea(spark.read.table(\"ea\").toPandas())\n",
    "    bucket[\"MCMR\"] = organize_mcmr(spark.read.table(\"mcmr\").toPandas())\n",
    "    bucket[\"PCCE\"] = organize_protracted(spark.read.table(\"pcce\").toPandas())\n",
    "    bucket[\"DREF_ESCALATED\"] = organize_dref_escalated(spark.read.table(\"dref_two\").toPandas())\n",
    "    return bucket\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def full_list(cols):\n",
    "    # For each indicator column, build the related set of columns:\n",
    "    # [status, days difference, actual date, expected date].\n",
    "    output = []\n",
    "    if not isinstance(cols, list):\n",
    "        cols = [cols]\n",
    "    for col in cols:\n",
    "        output.append(col)\n",
    "        output.append(f\"{col} (days)\")\n",
    "        output.append(f\"{col} date\")\n",
    "        output.append(f\"{col} expected date\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def replace_category(row, df_classification):\n",
    "    # If a row is classified as 'Orange', set 'Not Achieved' cells to 'N/A' (ignore them),\n",
    "    # otherwise leave the row as-is.\n",
    "    idx = row.name\n",
    "    if df_classification[idx] == 'Orange':\n",
    "        return row.apply(lambda x: 'N/A' if x == 'Not Achieved' else x)\n",
    "    return row\n",
    "\n",
    "\n",
    "def correct_im(df, df_classification):\n",
    "    # Apply replace_category to all rows in the Information Management (IM) dataframe.\n",
    "    df = df.copy()\n",
    "    df = df.apply(lambda row: replace_category(row, df_classification), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_df(df):\n",
    "    # For each row, count how many indicators fall into each category\n",
    "    # and compute \"Data Completeness\" and \"General Performance\" scores.\n",
    "    df = df.copy()\n",
    "    categories = [\"Achieved\", \"Not Achieved\", \"Achieved Early\", \"Achieved Late\", \"N/A\", \"Upcoming\"]\n",
    "\n",
    "    # Count how many times each status appears per row\n",
    "    for category in categories:\n",
    "        df.loc[:, category] = df.apply(lambda x: sum(str(cell) == category for cell in x), axis=1)\n",
    "    \n",
    "    # Data completeness = how many achieved vs total that could be achieved\n",
    "    dc_num = df[\"Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]\n",
    "    dc_den = df[\"Achieved\"] + df[\"Not Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]\n",
    "    df[\"Data Completeness\"] = np.where(dc_den != 0, dc_num / dc_den, 1)\n",
    "\n",
    "    # General performance = weighted score of outcome types\n",
    "    numerator = df[\"Achieved\"] * 3 + df[\"Achieved Early\"] * 4 + df[\"Achieved Late\"] * 2\n",
    "    denominator = (df[\"Achieved\"] + df[\"Not Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]) * 4\n",
    "    df[\"General Performance\"] = np.where(denominator != 0, numerator / denominator, 0)\n",
    "\n",
    "    # Move summary columns to the front\n",
    "    cols_to_move = [\"Achieved\", \"Not Achieved\", \"Upcoming\", \"Achieved Early\", \"Achieved Late\", \"N/A\", \"Data Completeness\", \"General Performance\"]\n",
    "    df = df[cols_to_move + [col for col in df.columns if col not in cols_to_move]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_general_info(folder, general):\n",
    "    # Combine summary stats from all areas (like Assessment, Logistics...) into one \"general\" overview table.\n",
    "    df = general.copy()\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    df.rename(columns={df.columns[1]: \"Trigger Date\"}, inplace=True)\n",
    "    for name, temp in folder.items():\n",
    "        df[\"Achieved\"] = temp[\"Achieved\"] if \"Achieved\" not in df.columns else df[\"Achieved\"] + temp[\"Achieved\"]\n",
    "        df[\"Not Achieved\"] = temp[\"Not Achieved\"] if \"Not Achieved\" not in df.columns else df[\"Not Achieved\"] + temp[\"Not Achieved\"]\n",
    "        df[\"Upcoming\"] = temp[\"Upcoming\"] if \"Upcoming\" not in df.columns else df[\"Upcoming\"] + temp[\"Upcoming\"]\n",
    "        df[\"Achieved Early\"] = temp[\"Achieved Early\"] if \"Achieved Early\" not in df.columns else df[\"Achieved Early\"] + temp[\"Achieved Early\"]\n",
    "        df[\"Achieved Late\"] = temp[\"Achieved Late\"] if \"Achieved Late\" not in df.columns else df[\"Achieved Late\"] + temp[\"Achieved Late\"]\n",
    "        df[\"N/A\"] = temp[\"N/A\"] if \"N/A\" not in df.columns else df[\"N/A\"] + temp[\"N/A\"]\n",
    "    \n",
    "    # Recompute completeness and performance at the general level\n",
    "    dc_num = df[\"Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]\n",
    "    dc_den = df[\"Achieved\"] + df[\"Not Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]\n",
    "    df[\"Data Completeness\"] = np.where(dc_den != 0, dc_num / dc_den, 1)\n",
    "    \n",
    "    numerator = df[\"Achieved\"] * 3 + df[\"Achieved Early\"] * 4 + df[\"Achieved Late\"] * 2\n",
    "    denominator = (df[\"Achieved\"] + df[\"Not Achieved\"] + df[\"Achieved Early\"] + df[\"Achieved Late\"]) * 4\n",
    "    df[\"General Performance\"] = np.where(denominator != 0, numerator / denominator, 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def area_split_ea(overview, columns, general):\n",
    "    # For EA data, slice the big \"achievements\" table into thematic areas\n",
    "    # (Assessment, Planning, Surge, HR, etc.), summarize each, and add General Information.\n",
    "    assessment = overview[full_list(columns[11])]\n",
    "    resource_mobilization = overview[full_list(columns[12:16] + [columns[22]] + columns[79:81])] # add EA coverage\n",
    "    surge = overview[full_list(columns[23:26])] # add % related values to the surge (rrp)\n",
    "    hr = overview[full_list(columns[38:40])] # add % related values to the hr (rrp)\n",
    "    coordination = overview[full_list(columns[40:44])] # Upcoming joint statement in master data\n",
    "    logistics = overview[full_list(columns[44:47])]\n",
    "    # im = overview[full_list(columns[47:52] + columns[71:73])]\n",
    "    im = overview[full_list(columns[47:51])]\n",
    "    risk = overview[full_list(columns[73:75])]\n",
    "    finance = overview[full_list(columns[52:56] + columns[77:79])]\n",
    "    program_delivery = overview[full_list(columns[56:58])]\n",
    "    security = overview[full_list(columns[69:70] + columns[76:77])]\n",
    "\n",
    "    areas = {}\n",
    "    areas[\"Assessment\"] = summarize_df(assessment)\n",
    "    areas[\"Planning\"] = summarize_df(resource_mobilization)\n",
    "    areas[\"Risk\"] = summarize_df(risk)\n",
    "    areas[\"Surge\"] = summarize_df(surge)\n",
    "    areas[\"HR\"] = summarize_df(hr)\n",
    "    areas[\"Coordination\"] = summarize_df(coordination)\n",
    "    areas[\"Logistics\"] = summarize_df(logistics)\n",
    "    areas[\"Information Management\"] = summarize_df(correct_im(im, general[\"Classification_\"]))\n",
    "    areas[\"Finance\"] = summarize_df(finance)\n",
    "    areas[\"Security\"] = summarize_df(security)\n",
    "    areas[\"Program Delivery\"] = summarize_df(program_delivery)\n",
    "    general_info = update_general_info(areas, general)\n",
    "    areas[\"General Information\"] = general_info\n",
    "    return areas\n",
    "\n",
    "def area_split_dref_escalated(overview, columns, general):\n",
    "    # For escalated DREF data, split indicators into areas and summarize them.\n",
    "    assessment = overview[full_list(columns[11])]\n",
    "    resource_mobilization = overview[full_list(columns[12:14])]\n",
    "    surge = overview[full_list(columns[14:17])]\n",
    "    logistics = overview[full_list(columns[30:32])]\n",
    "    finance = overview[full_list(columns[32:36])]\n",
    "    delivery = overview[full_list([columns[36]])]\n",
    "\n",
    "    if \"Tracker Status\" in general.columns:\n",
    "        general = general.rename(columns={\"Tracker Status\" : \"tracker Status\"})\n",
    "\n",
    "    areas = {}\n",
    "    \n",
    "    areas[\"Assessment\"] = summarize_df(assessment)\n",
    "    areas[\"Planning\"] = summarize_df(resource_mobilization)\n",
    "    areas[\"Surge\"] = summarize_df(surge)\n",
    "    areas[\"Logistics\"] = summarize_df(logistics)\n",
    "    areas[\"Finance\"] = summarize_df(finance)\n",
    "    areas[\"Program Delivery\"] = summarize_df(delivery)\n",
    "    general_info = update_general_info(areas, general)\n",
    "    areas[\"General Information\"] = general_info\n",
    "    return areas\n",
    "\n",
    "def area_split_dref(overview, columns, general):\n",
    "    # For regular DREF data, split the \"achievements\" table into areas and compute summaries.\n",
    "    assessment = overview[full_list(columns[11])]\n",
    "    risk = overview[full_list(columns[12:14])]\n",
    "    resource_mobilization = overview[full_list(columns[14:16])]\n",
    "    surge = overview[full_list(columns[16:19])]\n",
    "    logistics = overview[full_list(columns[32:34])]\n",
    "    finance = overview[full_list(columns[34:38] + columns[54:56])]\n",
    "    delivery = overview[full_list([columns[38]])] # add targeted population, ehi distribution, and implementation rate\n",
    "    security = overview[full_list(columns[51:52] + columns[53:54])]\n",
    "\n",
    "    areas = {}\n",
    "    \n",
    "    areas[\"Assessment\"] = summarize_df(assessment)\n",
    "    areas[\"Planning\"] = summarize_df(resource_mobilization)\n",
    "    areas[\"Surge\"] = summarize_df(surge)\n",
    "    areas[\"Risk\"] = summarize_df(risk)\n",
    "    areas[\"Logistics\"] = summarize_df(logistics)\n",
    "    areas[\"Finance\"] = summarize_df(finance)\n",
    "    areas[\"Program Delivery\"] = summarize_df(delivery) \n",
    "    areas[\"Security\"] = summarize_df(security)\n",
    "    general_info = update_general_info(areas, general)\n",
    "    areas[\"General Information\"] = general_info\n",
    "    return areas\n",
    "\n",
    "def area_split_mcmr(overview, columns, general):\n",
    "    # For multi-country (MCMR), split achievements into areas and summarize each.\n",
    "    resource_mobilization = overview[full_list(columns[11:14] + [columns[20]])] # add coverage\n",
    "    surge = overview[full_list(columns[21:23])] # add % related values to the surge (rrp)\n",
    "    hr = overview[full_list(columns[36:38])] # add % related values to the hr (rrp)\n",
    "    coordination = overview[full_list(columns[38])]\n",
    "    logistics = overview[full_list(columns[39:42])]\n",
    "    # im = overview[full_list(columns[42:43] + columns[47:53])]\n",
    "    im = overview[full_list(columns[42:43] + columns[47:50])]\n",
    "    finance = overview[full_list(columns[43:45])]\n",
    "\n",
    "    areas = {}\n",
    "    \n",
    "    areas[\"Planning\"] = summarize_df(resource_mobilization)\n",
    "    areas[\"Surge\"] = summarize_df(surge)\n",
    "    areas[\"HR\"] = summarize_df(hr)\n",
    "    areas[\"Coordination\"] = summarize_df(coordination)\n",
    "    areas[\"Logistics\"] = summarize_df(logistics)\n",
    "    areas[\"Information Management\"] = summarize_df(correct_im(im, general[\"Classification_\"]))\n",
    "    areas[\"Finance\"] = summarize_df(finance)\n",
    "    general_info = update_general_info(areas, general)\n",
    "    areas[\"General Information\"] = general_info\n",
    "    return areas\n",
    "\n",
    "\n",
    "def area_split_pcce(overview, columns, general):\n",
    "    # For protracted crisis (PCCE), split achievements into areas and summarize each.\n",
    "    assessment = overview[columns[11:12]]\n",
    "    resource_mobilization = overview[full_list(columns[12:15] + [columns[21]] + [columns[62]])]\n",
    "    surge = overview[full_list(columns[22:25])]\n",
    "    hr = overview[full_list(columns[39:41])]\n",
    "    coordination = overview[full_list(columns[41:44])]\n",
    "    logistics = overview[full_list(columns[44:47])]\n",
    "    # im = overview[full_list(columns[47:52] + columns[63:65])]\n",
    "    im = overview[full_list(columns[47:51])]\n",
    "\n",
    "    finance = overview[full_list(columns[52:56])]\n",
    "    # delivery = overview[full_list(columns[55:57])] # add percentage of targeted population receiving assistance and % of planned budget implementation\n",
    "    security = overview[full_list(columns[60:62])]\n",
    "\n",
    "    areas = {}\n",
    "    \n",
    "    areas[\"Assessment\"] = summarize_df(assessment)\n",
    "    areas[\"Planning\"] = summarize_df(resource_mobilization)\n",
    "    areas[\"Surge\"] = summarize_df(surge)\n",
    "    areas[\"HR\"] = summarize_df(hr)\n",
    "    areas[\"Coordination\"] = summarize_df(coordination)\n",
    "    areas[\"Logistics\"] = summarize_df(logistics)\n",
    "    areas[\"Information Management\"] = summarize_df(correct_im(im, general[\"Classification_\"]))\n",
    "    areas[\"Finance\"] = summarize_df(finance)\n",
    "    areas[\"Security\"] = summarize_df(security)\n",
    "    general_info = update_general_info(areas, general)\n",
    "    areas[\"General Information\"] = general_info\n",
    "    return areas\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def convert_date(date_str):\n",
    "    # Try to turn a messy date string into a proper datetime object.\n",
    "    # If it's a known placeholder like \"-\", \"DNU\", \"Not Achieved\", or \"N/A\", just return it as-is.\n",
    "    if date_str in [\"-\", \"DNU\", \"Not Achieved\", \"N/A\"] or pd.isna(date_str):\n",
    "        return date_str\n",
    "\n",
    "    date_formats = [\"%Y-%m-%d\", \"%d-%m-%Y\", \"%m-%d-%Y\", \"%m/%d/%Y\", \"%d/%m/%Y\"]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(str(date_str)[:10], fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "\n",
    "def determine_status_ea(row, l1, l2):\n",
    "    # For EA indicators, decide if a task is:\n",
    "    # \"Upcoming\", \"Not Achieved\", \"N/A\", \"Achieved Early\", \"Achieved\", or \"Achieved Late\"\n",
    "    # based on start date, completion date, and deadline rules.\n",
    "    keys = row.index.tolist()\n",
    "    r0, r1 = row.iloc[0], row.iloc[1]  # r0 = start date, r1 = completion or status\n",
    "    # If the third value in the row is \"Yes\", use the second limit (l2), else use first limit (l1)\n",
    "    if row.iloc[2] == \"Yes\":\n",
    "        limit = l2\n",
    "    else:\n",
    "        limit = l1\n",
    "    expected_date = r0 + pd.Timedelta(days=limit)\n",
    "    if r1 == \"-\" or pd.isna(r1):\n",
    "        # No completion date: check if deadline passed or not\n",
    "        deadline = r0 + pd.Timedelta(days=limit + 1)\n",
    "        if deadline >= datetime.now():\n",
    "            return pd.Series([\"Upcoming\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"]) \n",
    "        return pd.Series([\"Not Achieved\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    if r1 == \"DNU\" or r1 == \"N/A\":\n",
    "        # Explicitly marked as not applicable\n",
    "        return pd.Series([\"N/A\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    \n",
    "    if r1 == \"Not Achieved\":\n",
    "        # Explicitly marked as not achieved\n",
    "        return pd.Series([\"Not Achieved\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    \n",
    "    # Small tweak to treat 30 days as 31, probably because of month-length edge cases\n",
    "    if (limit == 30):\n",
    "        limit = 31\n",
    "    days = (r1 - r0).days\n",
    "    delta = days - limit\n",
    "    if days > limit:\n",
    "        return pd.Series([\"Achieved Late\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    if delta == 0:\n",
    "        return pd.Series([\"Achieved\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "\n",
    "    return pd.Series([\"Achieved Early\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "\n",
    "def determine_status(row, limit):\n",
    "    # Same as determine_status_ea but for non-EA logic (single deadline limit).\n",
    "    keys = row.index.tolist()\n",
    "    r0, r1 = row.iloc[0], row.iloc[1]\n",
    "    expected_date = r0 + pd.Timedelta(days=limit)\n",
    "    if r1 == \"-\" or pd.isna(r1):\n",
    "        deadline = r0 + pd.Timedelta(days=limit + 1)\n",
    "        if deadline >= datetime.now():\n",
    "            return pd.Series([\"Upcoming\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"]) \n",
    "        return pd.Series([\"Not Achieved\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    if r1 == \"DNU\" or r1 == \"N/A\":\n",
    "        return pd.Series([\"N/A\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    \n",
    "    if r1 == \"Not Achieved\":\n",
    "        return pd.Series([\"Not Achieved\", 365, \"-\", expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    \n",
    "    if (limit == 30):\n",
    "        limit = 31\n",
    "    days = (r1 - r0).days\n",
    "    delta = days - limit\n",
    "    if days > limit:\n",
    "        return pd.Series([\"Achieved Late\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "    if delta == 0:\n",
    "        return pd.Series([\"Achieved\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "\n",
    "    return pd.Series([\"Achieved Early\", delta, r1, expected_date], index=[keys[1], f\"{keys[1]} (days)\", f\"{keys[1]} date\", f\"{keys[1]} expected date\"])\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def process_ea(ea):\n",
    "    # Take organized EA data (disasters + operational progress) and compute detailed\n",
    "    # achievement status columns for each indicator with deadlines.\n",
    "    op = ea[\"operational_progresses\"].copy()\n",
    "    if op.empty:\n",
    "        return None\n",
    "    key = \"achievements\"\n",
    "\n",
    "    ea[key] = pd.DataFrame()\n",
    "    if \"Trigger Date_\" in ea[\"disasters\"].columns:\n",
    "        ea[\"disasters\"] = ea[\"disasters\"].rename(columns={\"Trigger Date_\":\"Trigger_Date_\"})\n",
    "    start_date = ea[\"disasters\"][\"Trigger_Date_\"].apply(convert_date)\n",
    "    # Hard-coded correction for one specific reference\n",
    "    start_date[\"EANigeriaMDRNG042\"] = ea[\"disasters\"][\"Launch Date\"][\"EANigeriaMDRNG042\"]\n",
    "    launch_date = ea[\"disasters\"][\"Launch Date\"].apply(convert_date)\n",
    "    surge_date = ea[\"disasters\"][\"Trigger_Date_\"].copy()\n",
    "    # Override surge dates with SURGE table where available\n",
    "    for ref, val in surge_requests.iterrows():\n",
    "        surge_date[ref] = val[\"requested-on\"]\n",
    "    surge_date = surge_date.apply(convert_date)\n",
    "\n",
    "    on = op.columns\n",
    "    for col in on:\n",
    "        op[col] = op[col].apply(convert_date)\n",
    "    \n",
    "    ea[key][\"Ref\"] = ea[\"disasters\"].index\n",
    "    ea[key].set_index(\"Ref\", inplace=True)\n",
    "    # Predefined index positions that behave differently for deadlines\n",
    "    points = [4, 5, 9, 10, 15, 16, 23, 24, 25, 26, 27, 28, 32, 33]\n",
    "    # Deltas define how many days each indicator has to be completed\n",
    "    deltas = [3, 3, 0, 4, 11, 18, 1, 2, 3, 11, 13, 2, 4, 7, 1, 11, 11, 14, 1, 3, 7, 14, 30, 11, 14, 16, 20, 32, 18, 7, 60, 90, 18, 34, 7, 2, 4, 60, 7]\n",
    "    deltas_b = [3, 3, 0, 0, 7, 14, 1, 2, 3, 7, 9, 2, 4, 7, 1, 7, 7, 10, 1, 3, 7, 14, 30, 7, 10, 12, 16, 29, 14, 7, 60, 90, 14, 30, 7, 2, 4, 60, 3]  \n",
    "    for i in range(len(deltas)):\n",
    "        if \"Surge\" in on[i] or \"RR\" in on[i]:\n",
    "            # Some surge tasks are counted from launch date, others from surge date\n",
    "            if i in points:\n",
    "                ea[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.concat([launch_date, op[on[i]], ea[\"dref_shift\"]], axis=1).apply(determine_status_ea, args=(deltas[i] - 4,deltas_b[i] - 4,), axis=1)\n",
    "            else:\n",
    "                ea[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.concat([surge_date, op[on[i]], ea[\"dref_shift\"]], axis=1).apply(determine_status_ea, args=(deltas[i],deltas_b[i],), axis=1)\n",
    "        else:\n",
    "            # Non-surge tasks start counting from launch or trigger date\n",
    "            if i in points:\n",
    "                ea[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.concat([launch_date, op[on[i]], ea[\"dref_shift\"]], axis=1).apply(determine_status_ea, args=(deltas[i] - 4,deltas_b[i] - 4,), axis=1)\n",
    "            else:\n",
    "                ea[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.concat([start_date, op[on[i]], ea[\"dref_shift\"]], axis=1).apply(determine_status_ea, args=(deltas[i],deltas_b[i],), axis=1)\n",
    "    return ea\n",
    "\n",
    "def process_dref(dref):\n",
    "    # Same idea as process_ea but for DREF data, using its own rules and deadlines.\n",
    "    op = dref[\"operational_progresses\"].copy()\n",
    "    \n",
    "    if op.empty:\n",
    "        return None\n",
    "    \n",
    "    key = \"achievements\"\n",
    "    dref[key] = pd.DataFrame()\n",
    "    start_date = dref[\"disasters\"][\"Trigger Date_\"].apply(convert_date)\n",
    "    surge_date = dref[\"disasters\"][\"Trigger Date_\"].copy()\n",
    "    for ref, val in surge_requests.iterrows():\n",
    "        surge_date[ref] = val[\"requested-on\"]\n",
    "    \n",
    "    surge_date = surge_date.apply(convert_date)\n",
    "    launch_date = dref[\"disasters\"][\"Launch Date\"].apply(convert_date)\n",
    "    on = op.columns\n",
    "    for col in on:\n",
    "        op[col] = op[col].apply(convert_date)\n",
    "    \n",
    "    \n",
    "    dref[key][\"Ref\"] = dref[\"disasters\"].index\n",
    "    dref[key].set_index(\"Ref\", inplace=True)\n",
    "    \n",
    "    points = [1, 8, 9, 10, 11, 12, 13, 14]\n",
    "    deltas = [3, 12, 14, 14, 14, 1, 2, 3, 19, 22, 17, 21, 22, 30, 30, 7, 7, 2, 4]\n",
    "    for i in range(len(deltas)):\n",
    "        if \"Surge\" in on[i] or \"RR\" in on[i]:\n",
    "            if i in points:\n",
    "                dref[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:\n",
    "                dref[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(surge_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "        else:\n",
    "            if i in points:\n",
    "                dref[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:\n",
    "                dref[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(start_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "\n",
    "    return dref\n",
    "\n",
    "def process_dref_escalated(dref_escalated):\n",
    "    # Process escalated DREF data, computing achievement status per indicator using its deadline setup.\n",
    "    op = dref_escalated[\"operational_progresses\"].copy()\n",
    "    \n",
    "    if op.empty:\n",
    "        return None\n",
    "    \n",
    "    key = \"achievements\"\n",
    "    dref_escalated[key] = pd.DataFrame()\n",
    "    start_date = dref_escalated[\"disasters\"][\"Trigger Date_\"].apply(convert_date)\n",
    "    surge_date = dref_escalated[\"disasters\"][\"Trigger Date_\"].copy()\n",
    "    for ref, val in surge_requests.iterrows():\n",
    "        surge_date[ref] = val[\"requested-on\"]\n",
    "    launch_date = dref_escalated[\"disasters\"][\"Launch Date\"].apply(convert_date)\n",
    "    surge_date = surge_date.apply(convert_date)\n",
    "\n",
    "    on = op.columns\n",
    "    for col in on:\n",
    "        op[col] = op[col].apply(convert_date)\n",
    "    \n",
    "    \n",
    "    dref_escalated[key][\"Ref\"] = dref_escalated[\"disasters\"].index\n",
    "    dref_escalated[key].set_index(\"Ref\", inplace=True)\n",
    "\n",
    "    deltas = [3, 10, 10, 1, 2, 3, 11, 14, 14, 6, 7, 10, 34]\n",
    "    for i in range(len(deltas)):\n",
    "        if \"Surge\" in on[i] or \"RR\" in on[i]:\n",
    "            if i > 6:\n",
    "                dref_escalated[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:\n",
    "                dref_escalated[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(surge_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "        else:\n",
    "            if i > 6:\n",
    "                dref_escalated[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:    \n",
    "                dref_escalated[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(start_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "\n",
    "    return dref_escalated\n",
    "\n",
    "def process_mcmr(mcmr):\n",
    "    # Process multi-country (MCMR) data: compute status for each operational indicator.\n",
    "    op = mcmr[\"operational_progresses\"].copy()\n",
    "    \n",
    "    if op.empty:\n",
    "        return None\n",
    "    key = \"achievements\"\n",
    "\n",
    "    mcmr[key] = pd.DataFrame()\n",
    "    start_date = mcmr[\"disasters\"][mcmr[\"disasters\"].columns[1]].apply(convert_date)\n",
    "\n",
    "    surge_date = mcmr[\"disasters\"][mcmr[\"disasters\"].columns[1]].copy()\n",
    "    for ref, val in surge_requests.iterrows():\n",
    "        surge_date[ref] = val[\"requested-on\"]\n",
    "    surge_date = surge_date.apply(convert_date)\n",
    "    launch_date = mcmr[\"disasters\"][\"Launch Date\"].apply(convert_date)\n",
    "    on = op.columns\n",
    "    for col in on:\n",
    "        op[col] = op[col].apply(convert_date)\n",
    "    \n",
    "    mcmr[key][\"Ref\"] = mcmr[\"disasters\"].index\n",
    "    mcmr[key].set_index(\"Ref\", inplace=True)\n",
    "    points = [2, 3, 6, 7, 9, 10, 11, 13, 14]\n",
    "    deltas = [3, 4, 11, 18, 1, 2, 11, 13, 1, 11, 11, 14, 1, 9, 14, 3, 7, 14, 30, 60, 90]\n",
    "\n",
    "    for i in range(len(deltas)):\n",
    "        if \"Surge\" in on[i] or \"RR\" in on[i]:\n",
    "            if i in points:\n",
    "                mcmr[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:\n",
    "                mcmr[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(surge_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "        else:\n",
    "            if i in points:\n",
    "                mcmr[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i] - 4,), axis=1)\n",
    "            else:\n",
    "                mcmr[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(start_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "    return mcmr\n",
    "    \n",
    "\n",
    "def process_pcce(pcce):\n",
    "    # Process protracted crisis (PCCE) data: compute deadlines and achievement status per indicator.\n",
    "    op = pcce[\"operational_progresses\"].copy()\n",
    "    if op.empty:\n",
    "        return None\n",
    "    key = \"achievements\"\n",
    "\n",
    "\n",
    "    pcce[key] = pd.DataFrame()\n",
    "    start_date = pcce[\"disasters\"][pcce[\"disasters\"].columns[1]].apply(convert_date)\n",
    "\n",
    "    surge_date = pcce[\"disasters\"][pcce[\"disasters\"].columns[1]].copy()\n",
    "    for ref, val in surge_requests.iterrows():\n",
    "        surge_date[ref] = val[\"requested-on\"]\n",
    "    surge_date = surge_date.apply(convert_date)\n",
    "\n",
    "    on = op.columns\n",
    "    for col in on:\n",
    "        op[col] = op[col].apply(convert_date)\n",
    "    \n",
    "    pcce[key][\"Ref\"] = pcce[\"disasters\"].index\n",
    "    pcce[key].set_index(\"Ref\", inplace=True)\n",
    "    deltas = [3, 3, 4, 11, 18, 7, 2, 3, 11, 13, 2, 7, 1, 11, 11, 14, 1, 3, 7, 14, 30, 11, 14, 16, 20, 32, 28, 7, 7, 6, 60, 90]\n",
    "    for i in range(len(deltas)):\n",
    "        if \"Surge\" in on[i] or \"RR\" in on[i]:\n",
    "            if i > 5:\n",
    "                pcce[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(launch_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "            else:\n",
    "                pcce[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(surge_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "        else:   \n",
    "            pcce[key][[on[i], f\"{on[i]} (days)\", f\"{on[i]} date\", f\"{on[i]} expected date\"]] = pd.merge(start_date, op[on[i]], left_index=True, right_index=True).apply(determine_status, args=(deltas[i],), axis=1)\n",
    "\n",
    "    return pcce\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def generate_overview(bucket, sheets):\n",
    "    # Run the processing functions for each operation type (EA, DREF, etc.),\n",
    "    # then split achievements into areas and return all the area-level data.\n",
    "    ea = process_ea(bucket[\"EA\"])\n",
    "    dref = process_dref(bucket[\"DREF\"])\n",
    "    mcmr = process_mcmr(bucket[\"MCMR\"])\n",
    "    pcce = process_pcce(bucket[\"PCCE\"])\n",
    "    dref_escalated = process_dref_escalated(bucket[\"DREF_ESCALATED\"])\n",
    "\n",
    "    key = \"achievements\"\n",
    "    split_dict = {}\n",
    "    for sheet_name, sheet in sheets.items():\n",
    "        if \"EA\" in sheet_name and ea != None:\n",
    "            split_dict[\"EA\"] = area_split_ea(ea[key], sheet.columns.tolist(), bucket[\"EA\"][\"disasters\"])\n",
    "        elif \"DREF_ESCALATED\" in sheet_name and dref_escalated != None:\n",
    "            split_dict[\"DREF_ESCALATED\"] = area_split_dref_escalated(dref_escalated[key], sheet.columns.tolist(), bucket[\"DREF_ESCALATED\"][\"disasters\"])\n",
    "        elif \"DREF\" in sheet_name and dref != None:\n",
    "            split_dict[\"DREF\"] = area_split_dref(dref[key], sheet.columns.tolist(), bucket[\"DREF\"][\"disasters\"])\n",
    "        elif \"MCMR\" in sheet_name and mcmr != None:\n",
    "            split_dict[\"MCMR\"] = area_split_mcmr(mcmr[key], sheet.columns.tolist(), bucket[\"MCMR\"][\"disasters\"])\n",
    "        elif \"Protracted\" in sheet_name and pcce != None:\n",
    "            split_dict[\"PCCE\"] = area_split_pcce(pcce[key], sheet.columns.tolist(), bucket[\"PCCE\"][\"disasters\"])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(\"All complete\")\n",
    "    \n",
    "    return split_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "def read_area_info_folder(dfs):\n",
    "    # Flatten all area-level tables for a single operation type into one dataframe\n",
    "    # with standard columns (Ref, Area, Achieved, Not Achieved, etc.).\n",
    "    cols = [\"Ref\", \"Area\", \"Achieved\", \"Not Achieved\", \"Upcoming\", \"Achieved Early\", \"Achieved Late\", \"N/A\", \"Data Completeness\", \"General Performance\"]\n",
    "    df_list = []\n",
    "    for key, df in dfs.items():\n",
    "        df.reset_index(inplace=True)\n",
    "        df[\"Area\"] = key\n",
    "        if key == \"General Information\":\n",
    "            continue\n",
    "        df = df[cols]\n",
    "        df_list.append(df)\n",
    "    if len(df_list) == 0:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    else:    \n",
    "        df_combined = pd.concat(df_list)\n",
    "        return df_combined\n",
    "\n",
    "\n",
    "def area_info(area_split_dfs):\n",
    "    # Combine area info for all operation types (EA, DREF, MCMR, PCCE, DREF_ESCALATED) into a single dataframe.\n",
    "    df = read_area_info_folder(area_split_dfs.get(\"EA\", {}))\n",
    "    df1 = read_area_info_folder(area_split_dfs.get(\"DREF\", {}))\n",
    "    df2 = read_area_info_folder(area_split_dfs.get(\"MCMR\", {}))\n",
    "    df3 = read_area_info_folder(area_split_dfs.get(\"PCCE\", {}))\n",
    "    df4 = read_area_info_folder(area_split_dfs.get(\"DREF_ESCALATED\", {}))\n",
    "    df_combined = pd.concat([df, df1, df2, df3])\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "'''----------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "def calculate_delta(today, status, expected, d):\n",
    "    # Turn raw numeric or date-based lag info into human-readable text like\n",
    "    # \"5 days to deadline\", \"3 days behind deadline\", or \"Achieved 2 days early\".\n",
    "    if pd.notna(status) and status == \"N/A\":\n",
    "        delta = \"N/A\"\n",
    "    else:\n",
    "        if d == 90:\n",
    "            # For \"Upcoming\" style entries, use expected date vs today\n",
    "            if status == \"Upcoming\":\n",
    "                delta = expected - today\n",
    "                if delta.days == 0:\n",
    "                    delta = f\"Due Today\"\n",
    "                else:\n",
    "                    delta = f\"{delta.days} days to deadline\"\n",
    "            else:\n",
    "                delta = today - expected\n",
    "                delta = f\"{delta.days} days behind deadline\"\n",
    "        else:\n",
    "            # For completed tasks, d is a signed number of days early/late\n",
    "            delta = d * -1\n",
    "            if delta == 0:\n",
    "                delta = \"Achieved On Date\"\n",
    "            elif delta > 0:\n",
    "                delta = f\"Achieved {delta} days early\"\n",
    "            else:\n",
    "                delta = f\"Achieved {delta * -1} days late\"\n",
    "    return delta\n",
    "\n",
    "\n",
    "def read_task_info(df, op, op_df, area):\n",
    "    # From an area-level summary dataframe, build a row-per-task structure with status, dates,\n",
    "    # delta text, escalation info, etc., for one operation type.\n",
    "    cols = df.columns[9:].copy()\n",
    "    task_infos = []\n",
    "    today = pd.Timestamp(date.today())\n",
    "    for index, row in df.iterrows():\n",
    "        for a, b, c, d in zip(cols[::4], cols[1::4], cols[2::4], cols[3::4]):\n",
    "            e = a.replace(\"_\", \" \")\n",
    "            filtered_df = escalation[(escalation[\"Variant\"] == op.upper()) & (escalation[\"Indicator\"] == e)]\n",
    "            column_name = op_df[\"Appeal_Name_\"][index].split(\" \")[:-1]\n",
    "            column_name = \"_\".join(column_name)\n",
    "            column_name = column_name + \"__\" +op_df[\"Appeal_Code\"][index] + \"_\"\n",
    "            if not filtered_df.empty and (column_name in filtered_df.columns or f\"EA_{column_name}\" in filtered_df.columns):\n",
    "                escalated = filtered_df.loc[:, column_name].values[0]\n",
    "            else:\n",
    "                escalated = \"Not Escalated\"\n",
    "            status = row[a] if pd.notna(row[a]) else \"Not Achieved\"\n",
    "            if row[b] > 90:\n",
    "                raw_delta = 90\n",
    "            else:\n",
    "                raw_delta = row[b]\n",
    "            delta = calculate_delta(today, row[a], row[d], raw_delta)\n",
    "            task_infos.append({\n",
    "                \"Ref\" : row[\"Ref\"],\n",
    "                \"EWTS Varient\" : op,\n",
    "                \"Area\" : area,\n",
    "                \"Task\" : e,\n",
    "                \"Status\" : status,\n",
    "                \"Completed\" : str(row[c])[:10],\n",
    "                \"Expected Date\" : \"-\" if row[a] == \"N/A\" else str(row[d])[:10],\n",
    "                \"Delta\" : delta,\n",
    "                \"Raw_Delta\": raw_delta,\n",
    "                \"Escalated\" : escalated,\n",
    "            })\n",
    "    return task_infos\n",
    "\n",
    "\n",
    "def read_im(df, op, op_df):\n",
    "    # Same as read_task_info but specifically for the Information Management area,\n",
    "    # which uses a fixed indicator text to look up escalation.\n",
    "    cols = df.columns[9:].copy()\n",
    "    task_infos = []\n",
    "    today = pd.Timestamp(date.today())\n",
    "    for index, row in df.iterrows():\n",
    "        for a, b, c, d in zip(cols[::4], cols[1::4], cols[2::4], cols[3::4]):\n",
    "            if op == \"mcmr\":\n",
    "                filtered_df = escalation[(escalation[\"Variant\"] == op.upper()) & (escalation[\"Indicator\"] == \"A multi country dashboard is in place and updated timely to display the situation and the activities being implemented\")]\n",
    "            else:\n",
    "                filtered_df = escalation[(escalation[\"Variant\"] == op.upper()) & (escalation[\"Indicator\"] == \"A dashboard is in place and updated timely to display the situation and the activities being implemented\")]\n",
    "            \n",
    "            column_name = op_df[\"Appeal_Name_\"][index].split(\" \")[:-1]\n",
    "            column_name = \"_\".join(column_name)\n",
    "            column_name = column_name + \"__\" +op_df[\"Appeal_Code\"][index] + \"_\"\n",
    "            if not filtered_df.empty and (column_name in filtered_df.columns or f\"EA_{column_name}\" in filtered_df.columns):\n",
    "                escalated = filtered_df.loc[:, column_name].values[0]\n",
    "            else:\n",
    "                escalated = \"Not Escalated\"\n",
    "            status = row[a] if pd.notna(row[a]) else \"Not Achieved\"\n",
    "            if row[b] > 90:\n",
    "                raw_delta = 90\n",
    "            else:\n",
    "                raw_delta = row[b]\n",
    "            delta = calculate_delta(today, row[a], row[d], raw_delta)\n",
    "            task_infos.append({\n",
    "                \"Ref\" : row[\"Ref\"],\n",
    "                \"EWTS Varient\" : op,\n",
    "                \"Area\" : \"Information Management\",\n",
    "                \"Task\" : a.replace(\"_\", \" \"),\n",
    "                \"Status\" : status,\n",
    "                \"Completed\" : str(row[c])[:10],\n",
    "                \"Expected Date\": \"-\" if row[a] == \"N/A\" else str(row[d])[:10],\n",
    "                \"Delta\" : delta,\n",
    "                \"Raw_Delta\": raw_delta,\n",
    "                \"Escalated\" : escalated,\n",
    "            })\n",
    "    return task_infos\n",
    "\n",
    "\n",
    "status_mapping = {\n",
    "    # Map a status label to a numeric score for later aggregation.\n",
    "    \"Achieved\" : 75,\n",
    "    \"Achieved Early\" : 100,\n",
    "    \"Achieved Late\" : 50,\n",
    "    \"N/A\" : 0,\n",
    "    \"Upcoming\" : 0,\n",
    "    \"Not Achieved\" : 0,\n",
    "}\n",
    "\n",
    "\n",
    "def areas_in_op(adf, op):\n",
    "    # For a single operation type (e.g. EA or DREF),\n",
    "    # loop through all its areas and build a list of task info dictionaries.\n",
    "    task_infos = []\n",
    "    op_df = adf.get(\"General Information\", pd.DataFrame())\n",
    "    op_df = op_df[op_df.columns[:18]]\n",
    "    if \"Appeal_Name_\" not in op_df.columns:\n",
    "        op_df = op_df.rename(columns={\"Appeal Name_\": \"Appeal_Name_\", \"Appeal Code\":\"Appeal_Code\"})\n",
    "    \n",
    "    for key, df in adf.items():\n",
    "        if key == \"General Information\":\n",
    "            continue\n",
    "        elif key == \"Information Management\":\n",
    "            task_infos += read_im(df, op, op_df)\n",
    "            continue\n",
    "        task_infos += read_task_info(df, op, op_df, key)\n",
    "    return task_infos\n",
    "\n",
    "\n",
    "def task_info_extraction(area_split_dfs):\n",
    "    # Gather task-level info for all operation types into a single dataframe\n",
    "    # and attach a numeric score for each task based on its status.\n",
    "    task_infos = areas_in_op(area_split_dfs.get(\"EA\", {}), \"ea\") + areas_in_op(area_split_dfs.get(\"DREF\", {}), \"dref\") + areas_in_op(area_split_dfs.get(\"MCMR\", {}), \"mcmr\") + areas_in_op(area_split_dfs.get(\"PCCE\", {}), \"protracted crisis\") + areas_in_op(area_split_dfs.get(\"DREF_ESCALATED\", {}), \"dref_escalated\")\n",
    "    df = pd.DataFrame(task_infos)\n",
    "    df[\"Score\"] = df[\"Status\"].map(status_mapping)\n",
    "    df[\"Delta\"] = df[\"Delta\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ==== MAIN FLOW ====\n",
    "# From here down, we're not defining functions anymore.\n",
    "# This is the \"orchestrator\" that:\n",
    "# 1) pulls raw data from Spark\n",
    "# 2) organizes + processes it\n",
    "# 3) calculates summaries and scores\n",
    "# 4) writes the final cleaned tables back to Spark\n",
    "\n",
    "\n",
    "# 1) READ RAW TABLES FROM SPARK INTO PANDAS\n",
    "sheets = {}\n",
    "sheets[\"EA\"] = spark.read.table(\"ea\").toPandas()\n",
    "sheets[\"DREF\"] = spark.read.table(\"dref\").toPandas()\n",
    "dref_cols = sheets[\"DREF\"].columns\n",
    "sheets[\"MCMR\"] = spark.read.table(\"mcmr\").toPandas()\n",
    "sheets[\"Protracted\"] = spark.read.table(\"pcce\").toPandas()\n",
    "sheets[\"DREF_ESCALATED\"] = spark.read.table(\"dref_two\").toPandas()\n",
    "escalation = spark.read.table(\"escalation_events\").toPandas()\n",
    "surge_requests = spark.read.table(\"SURGE\").toPandas()\n",
    "surge_requests = surge_requests.set_index(\"Ref\")  # make \"Ref\" the key so we can look up surge dates fast\n",
    "\n",
    "# 2) ORGANIZE RAW SHEETS INTO STRUCTURED BUCKET (DISASTERS + OP PROGRESS + SHIFT INFO)\n",
    "bucket = organize_sheets()\n",
    "\n",
    "# 3) PROCESS EACH OPERATION TYPE TO BUILD \"ACHIEVEMENTS\" AND SPLIT THEM BY AREA\n",
    "area_split_dfs = generate_overview(bucket, sheets)\n",
    "\n",
    "# 4) BUILD GENERAL-LEVEL OVERVIEW TABLE ACROSS ALL OPS\n",
    "#    - Take \"General Information\" from every operation type\n",
    "#    - Stack them together into one big dataframe\n",
    "general_df = pd.concat([i[\"General Information\"] for _, i in area_split_dfs.items()])\n",
    "general_df.reset_index(inplace=True)\n",
    "\n",
    "# Make sure \"Ref\" is the first column, then keep first 20 columns\n",
    "general_df = general_df[['Ref'] + [col for col in general_df.columns[:20] if col != 'Ref']]\n",
    "\n",
    "# Normalize launch date to proper datetime\n",
    "general_df[\"Launch Date\"] = general_df[\"Launch Date\"].apply(convert_date)\n",
    "\n",
    "# 5) PUSH GENERAL INFO BACK TO SPARK AS A CLEAN TABLE\n",
    "spark_general_info = spark.createDataFrame(general_df)\n",
    "# Replace spaces with underscores in column names to avoid Spark whining\n",
    "spark_general_info = spark_general_info.toDF(*[c.replace(\" \", \"_\") for c in spark_general_info.columns])\n",
    "# Make sure date columns are actual date type in Spark\n",
    "spark_general_info = spark_general_info.withColumn(\"Trigger_Date\", to_date(\"Trigger_Date\", \"M/d/yyyy\"))\n",
    "spark_general_info = spark_general_info.withColumn(\"Launch_Date\", to_date(\"Launch_Date\", \"M/d/yyyy\"))\n",
    "# Overwrite the target Delta table with the new data\n",
    "spark_general_info.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"master_data_processing.general_info\")\n",
    "\n",
    "# 6) BUILD AREA-LEVEL SUMMARY TABLE (PER AREA, PER OPERATION)\n",
    "df_area_info = area_info(area_split_dfs)\n",
    "\n",
    "# 7) EXTRACT TASK-LEVEL INFO (ONE ROW PER TASK) FROM THE AREA SPLITS\n",
    "ti = task_info_extraction(area_split_dfs)\n",
    "\n",
    "# Clean trailing spaces from task names\n",
    "ti[\"Task\"] = ti[\"Task\"].str.rstrip()\n",
    "\n",
    "# Drop one very specific, noisy task row that you don't want in the output\n",
    "ti = ti[~ti[\"Task\"].str.contains(\"Working Advance Request  IRP Form  signed by IFRC and NS no longer than 2 days from the DREF approval\", na=False)]\n",
    "\n",
    "# 8) PUSH TASK-LEVEL TABLE TO SPARK\n",
    "spark_task_infos = spark.createDataFrame(ti)\n",
    "spark_task_infos = spark_task_infos.toDF(*[c.replace(\" \", \"_\") for c in spark_task_infos.columns])\n",
    "\n",
    "# For N/A or Upcoming tasks with score=0, remove the score (set to null) so they don't drag averages\n",
    "spark_task_infos = spark_task_infos.withColumn(\n",
    "    \"Score\",\n",
    "    when(((col(\"Status\") == \"N/A\") | (col(\"Status\") == \"Upcoming\")) & (col(\"Score\") == 0), None)\n",
    "    .otherwise(col(\"Score\"))\n",
    ")\n",
    "spark_task_infos.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"master_data_processing.task_info\")\n",
    "\n",
    "# 9) FINAL CLEANUP FOR AREA-LEVEL INFO BEFORE SENDING TO SPARK\n",
    "# Fill missing performance with 0 first\n",
    "df_area_info[\"General Performance\"] = df_area_info[\"General Performance\"].fillna(0)\n",
    "\n",
    "# If literally nothing was done in that area (all zeros), set performance back to None\n",
    "df_area_info.loc[\n",
    "    (df_area_info[\"Achieved\"] == 0) &\n",
    "    (df_area_info[\"Achieved Early\"] == 0) &\n",
    "    (df_area_info[\"Achieved Late\"] == 0) &\n",
    "    (df_area_info[\"Not Achieved\"] == 0),\n",
    "    \"General Performance\"\n",
    "] = None\n",
    "\n",
    "# 10) PUSH AREA-LEVEL SUMMARY TABLE TO SPARK\n",
    "spark_area_info = spark.createDataFrame(df_area_info)\n",
    "spark_area_info = spark_area_info.toDF(*[c.replace(\" \", \"_\") for c in spark_area_info.columns])\n",
    "\n",
    "spark_area_info.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"master_data_processing.area_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM task_info\")\n",
    "print(len(df.columns))\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
